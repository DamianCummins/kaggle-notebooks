{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Indians Diabetes Database\n",
    "\n",
    "### Context\n",
    "This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n",
    "\n",
    "###Â Content\n",
    "The datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n",
    "\n",
    "### Acknowledgements\n",
    "Smith, J.W., Everhart, J.E., Dickson, W.C., Knowler, W.C., & Johannes, R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press.\n",
    "\n",
    "### Inspiration\n",
    "Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./kernel150594f677.ipynb\n",
      "./diabetes.csv\n",
      "./logs/fit/20200718-194236/train/events.out.tfevents.1595097760.Damians-MBP.2607.886.v2\n",
      "./logs/fit/20200718-194236/train/events.out.tfevents.1595097761.Damians-MBP.profile-empty\n",
      "./logs/fit/20200718-194236/train/plugins/profile/2020-07-18_19-42-41/local.trace\n",
      "./logs/fit/20200718-194236/validation/events.out.tfevents.1595097761.Damians-MBP.2607.1106.v2\n",
      "./.ipynb_checkpoints/kernel150594f677-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('./'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./diabetes.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']]\n",
    "y = df['Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>62</td>\n",
       "      <td>10</td>\n",
       "      <td>278</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.881</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>1</td>\n",
       "      <td>193</td>\n",
       "      <td>50</td>\n",
       "      <td>16</td>\n",
       "      <td>375</td>\n",
       "      <td>25.9</td>\n",
       "      <td>0.655</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>86</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>30.1</td>\n",
       "      <td>0.143</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>11</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>37</td>\n",
       "      <td>150</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.785</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>78</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>39.4</td>\n",
       "      <td>0.236</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "279            2      108             62             10      278  25.3   \n",
       "258            1      193             50             16      375  25.9   \n",
       "249            1      111             86             19        0  30.1   \n",
       "740           11      120             80             37      150  42.3   \n",
       "725            4      112             78             40        0  39.4   \n",
       "\n",
       "     DiabetesPedigreeFunction  Age  \n",
       "279                     0.881   22  \n",
       "258                     0.655   24  \n",
       "249                     0.143   23  \n",
       "740                     0.785   48  \n",
       "725                     0.236   38  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279    0\n",
       "258    0\n",
       "249    0\n",
       "740    1\n",
       "725    0\n",
       "Name: Outcome, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7844827586206896\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for name,clf in zip(names, classifiers):\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_test, y_test)\n",
    "    scores[name] = score\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Process 0.8189655172413793\n",
      "Decision Tree 0.8189655172413793\n",
      "Linear SVM 0.8103448275862069\n",
      "Naive Bayes 0.8017241379310345\n",
      "Random Forest 0.7931034482758621\n",
      "QDA 0.7844827586206896\n",
      "AdaBoost 0.7758620689655172\n",
      "Neural Net 0.7413793103448276\n",
      "Nearest Neighbors 0.7241379310344828\n",
      "RBF SVM 0.6724137931034483\n"
     ]
    }
   ],
   "source": [
    "sorted_scores = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in sorted_scores:\n",
    "    print(i[0], i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - Tensorflow\n",
    "https://drive.google.com/file/d/1Y42Pic0ebkxOz2PrL7hjh3Yj2vE0MVHS/view - the paper associated with this dataset achieved a higher accuracy using a Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-2ddcdfdbe8d27bb6\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-2ddcdfdbe8d27bb6\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Keras TensorBoard callback.\n",
    "logdir=\"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "Input Layer -> Fully Connected 64 -> Dropout -> Fully Connected 32 -> Dropout -> Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(652, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  Dense(64, input_shape=(8,), activation='elu'),\n",
    "  Dropout(0.25),\n",
    "  Dense(32, activation='elu'),\n",
    "  Dropout(0.5),\n",
    "  Dense(1, activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='RMSProp',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0725 15:20:01.097973 4606760384 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 586 samples, validate on 66 samples\n",
      "Epoch 1/300\n",
      "586/586 [==============================] - 0s 518us/sample - loss: 0.5547 - accuracy: 0.4334 - val_loss: 0.5308 - val_accuracy: 0.4242\n",
      "Epoch 2/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.4755 - accuracy: 0.5051 - val_loss: 0.3165 - val_accuracy: 0.6212\n",
      "Epoch 3/300\n",
      "586/586 [==============================] - 0s 63us/sample - loss: 0.4238 - accuracy: 0.5631 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 4/300\n",
      "586/586 [==============================] - 0s 66us/sample - loss: 0.3854 - accuracy: 0.6075 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 5/300\n",
      "586/586 [==============================] - 0s 80us/sample - loss: 0.3663 - accuracy: 0.6246 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 6/300\n",
      "586/586 [==============================] - 0s 88us/sample - loss: 0.3701 - accuracy: 0.6229 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 7/300\n",
      "586/586 [==============================] - 0s 65us/sample - loss: 0.3661 - accuracy: 0.6297 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 8/300\n",
      "586/586 [==============================] - 0s 66us/sample - loss: 0.3518 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 9/300\n",
      "586/586 [==============================] - 0s 68us/sample - loss: 0.3554 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 10/300\n",
      "586/586 [==============================] - 0s 76us/sample - loss: 0.3504 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 11/300\n",
      "586/586 [==============================] - 0s 74us/sample - loss: 0.3557 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 12/300\n",
      "586/586 [==============================] - 0s 66us/sample - loss: 0.3559 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 13/300\n",
      "586/586 [==============================] - 0s 76us/sample - loss: 0.3567 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 14/300\n",
      "586/586 [==============================] - 0s 65us/sample - loss: 0.3626 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 15/300\n",
      "586/586 [==============================] - 0s 68us/sample - loss: 0.3587 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 16/300\n",
      "586/586 [==============================] - 0s 64us/sample - loss: 0.3546 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 17/300\n",
      "586/586 [==============================] - 0s 65us/sample - loss: 0.3531 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 18/300\n",
      "586/586 [==============================] - 0s 67us/sample - loss: 0.3596 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 19/300\n",
      "586/586 [==============================] - 0s 65us/sample - loss: 0.3567 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 20/300\n",
      "586/586 [==============================] - 0s 76us/sample - loss: 0.3544 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 21/300\n",
      "586/586 [==============================] - 0s 74us/sample - loss: 0.3614 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 22/300\n",
      "586/586 [==============================] - 0s 76us/sample - loss: 0.3528 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 23/300\n",
      "586/586 [==============================] - 0s 76us/sample - loss: 0.3575 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 24/300\n",
      "586/586 [==============================] - 0s 82us/sample - loss: 0.3583 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 25/300\n",
      "586/586 [==============================] - 0s 67us/sample - loss: 0.3552 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 26/300\n",
      "586/586 [==============================] - 0s 75us/sample - loss: 0.3565 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 27/300\n",
      "586/586 [==============================] - 0s 70us/sample - loss: 0.3549 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 28/300\n",
      "586/586 [==============================] - 0s 72us/sample - loss: 0.3506 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 29/300\n",
      "586/586 [==============================] - 0s 73us/sample - loss: 0.3502 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 30/300\n",
      "586/586 [==============================] - 0s 67us/sample - loss: 0.3586 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 31/300\n",
      "586/586 [==============================] - 0s 67us/sample - loss: 0.3573 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 32/300\n",
      "586/586 [==============================] - 0s 71us/sample - loss: 0.3552 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 33/300\n",
      "586/586 [==============================] - 0s 64us/sample - loss: 0.3603 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 34/300\n",
      "586/586 [==============================] - 0s 104us/sample - loss: 0.3569 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 35/300\n",
      "586/586 [==============================] - 0s 103us/sample - loss: 0.3552 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 36/300\n",
      "586/586 [==============================] - 0s 91us/sample - loss: 0.3541 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 37/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.3561 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 38/300\n",
      "586/586 [==============================] - 0s 58us/sample - loss: 0.3580 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 39/300\n",
      "586/586 [==============================] - 0s 54us/sample - loss: 0.3559 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 40/300\n",
      "586/586 [==============================] - 0s 56us/sample - loss: 0.3516 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 41/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.3546 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 42/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3503 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 43/300\n",
      "586/586 [==============================] - 0s 63us/sample - loss: 0.3535 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 44/300\n",
      "586/586 [==============================] - 0s 63us/sample - loss: 0.3552 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 45/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.3592 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 46/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.3605 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 47/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3564 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 48/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.3610 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 49/300\n",
      "586/586 [==============================] - 0s 60us/sample - loss: 0.3578 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 50/300\n",
      "586/586 [==============================] - 0s 54us/sample - loss: 0.3537 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 51/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.3585 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 52/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.3576 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 53/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.3519 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 54/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3614 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 55/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.3602 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 56/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3605 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 57/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.3535 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 58/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3559 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 59/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3637 - accuracy: 0.6348 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 60/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3593 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 61/300\n",
      "586/586 [==============================] - 0s 42us/sample - loss: 0.3637 - accuracy: 0.6348 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 62/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3524 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 63/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3560 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 64/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3597 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 65/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3523 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 66/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3563 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 67/300\n",
      "586/586 [==============================] - 0s 54us/sample - loss: 0.3635 - accuracy: 0.6331 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 68/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3530 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 69/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3579 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 70/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3641 - accuracy: 0.6331 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 71/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3523 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 72/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3619 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 73/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3530 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 74/300\n",
      "586/586 [==============================] - 0s 42us/sample - loss: 0.3541 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 75/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3421 - accuracy: 0.6587 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 76/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3679 - accuracy: 0.6297 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 77/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3544 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 78/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3548 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 79/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3606 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 80/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3476 - accuracy: 0.6519 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 81/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3653 - accuracy: 0.6314 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 82/300\n",
      "586/586 [==============================] - 0s 62us/sample - loss: 0.3555 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 83/300\n",
      "586/586 [==============================] - 0s 67us/sample - loss: 0.3559 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 84/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.3512 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 85/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3586 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 86/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.3469 - accuracy: 0.6519 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 87/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3505 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 88/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3515 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 89/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3569 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 90/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3483 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 91/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3567 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 92/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3691 - accuracy: 0.6297 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 93/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3579 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 94/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3492 - accuracy: 0.6502 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 95/300\n",
      "586/586 [==============================] - 0s 41us/sample - loss: 0.3562 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 96/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3577 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 97/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3506 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 98/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3567 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 99/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3515 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 100/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3514 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 101/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3472 - accuracy: 0.6536 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 102/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3583 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 103/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3575 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 104/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3543 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 105/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3471 - accuracy: 0.6502 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 106/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3603 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 107/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3551 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 108/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3530 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 109/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3524 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 110/300\n",
      "586/586 [==============================] - 0s 41us/sample - loss: 0.3567 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 111/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3449 - accuracy: 0.6519 - val_loss: 0.3182 - val_accuracy: 0.6818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3475 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 113/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3569 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 114/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3504 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 115/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3528 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 116/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.3527 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 117/300\n",
      "586/586 [==============================] - 0s 65us/sample - loss: 0.3465 - accuracy: 0.6468 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 118/300\n",
      "586/586 [==============================] - 0s 73us/sample - loss: 0.3565 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 119/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.3613 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 120/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.3443 - accuracy: 0.6519 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 121/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.3514 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 122/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3450 - accuracy: 0.6536 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 123/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3577 - accuracy: 0.6399 - val_loss: 0.3188 - val_accuracy: 0.6818\n",
      "Epoch 124/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.3480 - accuracy: 0.6468 - val_loss: 0.3262 - val_accuracy: 0.6667\n",
      "Epoch 125/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3419 - accuracy: 0.6536 - val_loss: 0.3330 - val_accuracy: 0.6667\n",
      "Epoch 126/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3582 - accuracy: 0.6365 - val_loss: 0.3287 - val_accuracy: 0.6667\n",
      "Epoch 127/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3607 - accuracy: 0.6348 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 128/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3580 - accuracy: 0.6399 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 129/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3573 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 130/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3538 - accuracy: 0.6433 - val_loss: 0.3183 - val_accuracy: 0.6818\n",
      "Epoch 131/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3608 - accuracy: 0.6348 - val_loss: 0.3183 - val_accuracy: 0.6818\n",
      "Epoch 132/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.3590 - accuracy: 0.6365 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 133/300\n",
      "586/586 [==============================] - 0s 57us/sample - loss: 0.3539 - accuracy: 0.6416 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 134/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.3432 - accuracy: 0.6536 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 135/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.3538 - accuracy: 0.6451 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 136/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.3557 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 137/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.3436 - accuracy: 0.6536 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 138/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3463 - accuracy: 0.6485 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 139/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3498 - accuracy: 0.6502 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 140/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3479 - accuracy: 0.6502 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 141/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3593 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 142/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3544 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 143/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3520 - accuracy: 0.6433 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 144/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3545 - accuracy: 0.6382 - val_loss: 0.3182 - val_accuracy: 0.6818\n",
      "Epoch 145/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3440 - accuracy: 0.6519 - val_loss: 0.3184 - val_accuracy: 0.6818\n",
      "Epoch 146/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3352 - accuracy: 0.6570 - val_loss: 0.3214 - val_accuracy: 0.6818\n",
      "Epoch 147/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.3457 - accuracy: 0.6433 - val_loss: 0.3306 - val_accuracy: 0.6667\n",
      "Epoch 148/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.3351 - accuracy: 0.6587 - val_loss: 0.3319 - val_accuracy: 0.6667\n",
      "Epoch 149/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3407 - accuracy: 0.6502 - val_loss: 0.3325 - val_accuracy: 0.6667\n",
      "Epoch 150/300\n",
      "586/586 [==============================] - 0s 62us/sample - loss: 0.3474 - accuracy: 0.6468 - val_loss: 0.3333 - val_accuracy: 0.6667\n",
      "Epoch 151/300\n",
      "586/586 [==============================] - 0s 64us/sample - loss: 0.3282 - accuracy: 0.6536 - val_loss: 0.3331 - val_accuracy: 0.6667\n",
      "Epoch 152/300\n",
      "586/586 [==============================] - 0s 69us/sample - loss: 0.3403 - accuracy: 0.6502 - val_loss: 0.3330 - val_accuracy: 0.6667\n",
      "Epoch 153/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3315 - accuracy: 0.6587 - val_loss: 0.3333 - val_accuracy: 0.6667\n",
      "Epoch 154/300\n",
      "586/586 [==============================] - 0s 42us/sample - loss: 0.3523 - accuracy: 0.6399 - val_loss: 0.3234 - val_accuracy: 0.6667\n",
      "Epoch 155/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3389 - accuracy: 0.6536 - val_loss: 0.3293 - val_accuracy: 0.6667\n",
      "Epoch 156/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3531 - accuracy: 0.6365 - val_loss: 0.3289 - val_accuracy: 0.6667\n",
      "Epoch 157/300\n",
      "586/586 [==============================] - 0s 42us/sample - loss: 0.3456 - accuracy: 0.6485 - val_loss: 0.3332 - val_accuracy: 0.6667\n",
      "Epoch 158/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.3520 - accuracy: 0.6331 - val_loss: 0.3332 - val_accuracy: 0.6667\n",
      "Epoch 159/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3584 - accuracy: 0.6263 - val_loss: 0.3332 - val_accuracy: 0.6667\n",
      "Epoch 160/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3285 - accuracy: 0.6638 - val_loss: 0.3692 - val_accuracy: 0.6212\n",
      "Epoch 161/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3479 - accuracy: 0.6263 - val_loss: 0.3518 - val_accuracy: 0.6364\n",
      "Epoch 162/300\n",
      "586/586 [==============================] - 0s 41us/sample - loss: 0.3346 - accuracy: 0.6485 - val_loss: 0.3523 - val_accuracy: 0.6364\n",
      "Epoch 163/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3329 - accuracy: 0.6451 - val_loss: 0.3427 - val_accuracy: 0.6515\n",
      "Epoch 164/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3598 - accuracy: 0.6143 - val_loss: 0.3518 - val_accuracy: 0.6364\n",
      "Epoch 165/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3417 - accuracy: 0.6433 - val_loss: 0.3383 - val_accuracy: 0.6515\n",
      "Epoch 166/300\n",
      "586/586 [==============================] - 0s 42us/sample - loss: 0.3193 - accuracy: 0.6587 - val_loss: 0.3501 - val_accuracy: 0.6212\n",
      "Epoch 167/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2988 - accuracy: 0.6741 - val_loss: 0.3331 - val_accuracy: 0.6515\n",
      "Epoch 168/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3413 - accuracy: 0.6485 - val_loss: 0.3493 - val_accuracy: 0.6515\n",
      "Epoch 169/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3347 - accuracy: 0.6399 - val_loss: 0.3668 - val_accuracy: 0.6212\n",
      "Epoch 170/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3485 - accuracy: 0.6229 - val_loss: 0.3635 - val_accuracy: 0.6212\n",
      "Epoch 171/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3416 - accuracy: 0.6382 - val_loss: 0.3516 - val_accuracy: 0.6515\n",
      "Epoch 172/300\n",
      "586/586 [==============================] - 0s 56us/sample - loss: 0.3211 - accuracy: 0.6570 - val_loss: 0.3653 - val_accuracy: 0.6212\n",
      "Epoch 173/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3217 - accuracy: 0.6570 - val_loss: 0.3359 - val_accuracy: 0.6515\n",
      "Epoch 174/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3214 - accuracy: 0.6468 - val_loss: 0.3234 - val_accuracy: 0.6667\n",
      "Epoch 175/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3315 - accuracy: 0.6502 - val_loss: 0.3360 - val_accuracy: 0.6667\n",
      "Epoch 176/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3225 - accuracy: 0.6604 - val_loss: 0.3217 - val_accuracy: 0.6515\n",
      "Epoch 177/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3237 - accuracy: 0.6416 - val_loss: 0.3258 - val_accuracy: 0.6515\n",
      "Epoch 178/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3146 - accuracy: 0.6570 - val_loss: 0.3348 - val_accuracy: 0.6364\n",
      "Epoch 179/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.3122 - accuracy: 0.6638 - val_loss: 0.3386 - val_accuracy: 0.6364\n",
      "Epoch 180/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3302 - accuracy: 0.6399 - val_loss: 0.3428 - val_accuracy: 0.6515\n",
      "Epoch 181/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3077 - accuracy: 0.6689 - val_loss: 0.3287 - val_accuracy: 0.6667\n",
      "Epoch 182/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.3240 - accuracy: 0.6416 - val_loss: 0.3188 - val_accuracy: 0.6667\n",
      "Epoch 183/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.3100 - accuracy: 0.6553 - val_loss: 0.3055 - val_accuracy: 0.6818\n",
      "Epoch 184/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.2942 - accuracy: 0.6689 - val_loss: 0.3120 - val_accuracy: 0.6515\n",
      "Epoch 185/300\n",
      "586/586 [==============================] - 0s 59us/sample - loss: 0.2921 - accuracy: 0.6706 - val_loss: 0.3172 - val_accuracy: 0.6364\n",
      "Epoch 186/300\n",
      "586/586 [==============================] - 0s 62us/sample - loss: 0.3102 - accuracy: 0.6536 - val_loss: 0.2995 - val_accuracy: 0.6515\n",
      "Epoch 187/300\n",
      "586/586 [==============================] - 0s 57us/sample - loss: 0.3206 - accuracy: 0.6160 - val_loss: 0.2812 - val_accuracy: 0.6970\n",
      "Epoch 188/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.3173 - accuracy: 0.6229 - val_loss: 0.3100 - val_accuracy: 0.6667\n",
      "Epoch 189/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2922 - accuracy: 0.6621 - val_loss: 0.2969 - val_accuracy: 0.6667\n",
      "Epoch 190/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2903 - accuracy: 0.6382 - val_loss: 0.2913 - val_accuracy: 0.6667\n",
      "Epoch 191/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2911 - accuracy: 0.6519 - val_loss: 0.2949 - val_accuracy: 0.6515\n",
      "Epoch 192/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.3020 - accuracy: 0.6382 - val_loss: 0.2846 - val_accuracy: 0.6818\n",
      "Epoch 193/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2714 - accuracy: 0.6741 - val_loss: 0.2842 - val_accuracy: 0.6818\n",
      "Epoch 194/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2709 - accuracy: 0.6536 - val_loss: 0.2869 - val_accuracy: 0.6818\n",
      "Epoch 195/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2794 - accuracy: 0.6672 - val_loss: 0.2925 - val_accuracy: 0.6515\n",
      "Epoch 196/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.3034 - accuracy: 0.6263 - val_loss: 0.2865 - val_accuracy: 0.6667\n",
      "Epoch 197/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2817 - accuracy: 0.6621 - val_loss: 0.2810 - val_accuracy: 0.6515\n",
      "Epoch 198/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2643 - accuracy: 0.6706 - val_loss: 0.2763 - val_accuracy: 0.6667\n",
      "Epoch 199/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2715 - accuracy: 0.6536 - val_loss: 0.2762 - val_accuracy: 0.6515\n",
      "Epoch 200/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2673 - accuracy: 0.6570 - val_loss: 0.2860 - val_accuracy: 0.6364\n",
      "Epoch 201/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2729 - accuracy: 0.6570 - val_loss: 0.2884 - val_accuracy: 0.6667\n",
      "Epoch 202/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.2660 - accuracy: 0.6706 - val_loss: 0.2905 - val_accuracy: 0.6515\n",
      "Epoch 203/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.2651 - accuracy: 0.6536 - val_loss: 0.2787 - val_accuracy: 0.6515\n",
      "Epoch 204/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.2754 - accuracy: 0.6331 - val_loss: 0.2675 - val_accuracy: 0.6515\n",
      "Epoch 205/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2544 - accuracy: 0.6655 - val_loss: 0.2781 - val_accuracy: 0.6364\n",
      "Epoch 206/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2469 - accuracy: 0.6655 - val_loss: 0.2803 - val_accuracy: 0.6515\n",
      "Epoch 207/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.2665 - accuracy: 0.6604 - val_loss: 0.2523 - val_accuracy: 0.7121\n",
      "Epoch 208/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.2498 - accuracy: 0.6792 - val_loss: 0.2577 - val_accuracy: 0.6970\n",
      "Epoch 209/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2579 - accuracy: 0.6519 - val_loss: 0.2700 - val_accuracy: 0.6667\n",
      "Epoch 210/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2540 - accuracy: 0.6724 - val_loss: 0.2610 - val_accuracy: 0.6667\n",
      "Epoch 211/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.2659 - accuracy: 0.6553 - val_loss: 0.2534 - val_accuracy: 0.6515\n",
      "Epoch 212/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2590 - accuracy: 0.6570 - val_loss: 0.2487 - val_accuracy: 0.6667\n",
      "Epoch 213/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2466 - accuracy: 0.6758 - val_loss: 0.2405 - val_accuracy: 0.6667\n",
      "Epoch 214/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.2627 - accuracy: 0.6382 - val_loss: 0.2394 - val_accuracy: 0.6667\n",
      "Epoch 215/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2396 - accuracy: 0.6570 - val_loss: 0.2495 - val_accuracy: 0.6818\n",
      "Epoch 216/300\n",
      "586/586 [==============================] - 0s 41us/sample - loss: 0.2411 - accuracy: 0.6604 - val_loss: 0.2492 - val_accuracy: 0.6515\n",
      "Epoch 217/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2606 - accuracy: 0.6536 - val_loss: 0.2407 - val_accuracy: 0.6818\n",
      "Epoch 218/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2317 - accuracy: 0.6724 - val_loss: 0.2452 - val_accuracy: 0.6667\n",
      "Epoch 219/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.2527 - accuracy: 0.6485 - val_loss: 0.2496 - val_accuracy: 0.6364\n",
      "Epoch 220/300\n",
      "586/586 [==============================] - 0s 61us/sample - loss: 0.2418 - accuracy: 0.6587 - val_loss: 0.2316 - val_accuracy: 0.6515\n",
      "Epoch 221/300\n",
      "586/586 [==============================] - 0s 56us/sample - loss: 0.2446 - accuracy: 0.6860 - val_loss: 0.2343 - val_accuracy: 0.6818\n",
      "Epoch 222/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 0s 52us/sample - loss: 0.2352 - accuracy: 0.6775 - val_loss: 0.2372 - val_accuracy: 0.6667\n",
      "Epoch 223/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.2547 - accuracy: 0.6553 - val_loss: 0.2452 - val_accuracy: 0.6818\n",
      "Epoch 224/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.2410 - accuracy: 0.6587 - val_loss: 0.2360 - val_accuracy: 0.6667\n",
      "Epoch 225/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2329 - accuracy: 0.6792 - val_loss: 0.2432 - val_accuracy: 0.6515\n",
      "Epoch 226/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2333 - accuracy: 0.6809 - val_loss: 0.2355 - val_accuracy: 0.6515\n",
      "Epoch 227/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2396 - accuracy: 0.6621 - val_loss: 0.2344 - val_accuracy: 0.6818\n",
      "Epoch 228/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2354 - accuracy: 0.6724 - val_loss: 0.2208 - val_accuracy: 0.7121\n",
      "Epoch 229/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2114 - accuracy: 0.6945 - val_loss: 0.2271 - val_accuracy: 0.6970\n",
      "Epoch 230/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2310 - accuracy: 0.6672 - val_loss: 0.2287 - val_accuracy: 0.6667\n",
      "Epoch 231/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2191 - accuracy: 0.6962 - val_loss: 0.2325 - val_accuracy: 0.6970\n",
      "Epoch 232/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2139 - accuracy: 0.6860 - val_loss: 0.2319 - val_accuracy: 0.6818\n",
      "Epoch 233/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.2109 - accuracy: 0.6911 - val_loss: 0.2318 - val_accuracy: 0.6970\n",
      "Epoch 234/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2291 - accuracy: 0.6502 - val_loss: 0.2306 - val_accuracy: 0.6818\n",
      "Epoch 235/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2209 - accuracy: 0.6775 - val_loss: 0.2313 - val_accuracy: 0.6667\n",
      "Epoch 236/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2171 - accuracy: 0.6997 - val_loss: 0.2299 - val_accuracy: 0.6970\n",
      "Epoch 237/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2178 - accuracy: 0.6928 - val_loss: 0.2226 - val_accuracy: 0.6970\n",
      "Epoch 238/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.2074 - accuracy: 0.6877 - val_loss: 0.2291 - val_accuracy: 0.6970\n",
      "Epoch 239/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2241 - accuracy: 0.6587 - val_loss: 0.2247 - val_accuracy: 0.6818\n",
      "Epoch 240/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.2090 - accuracy: 0.6945 - val_loss: 0.2233 - val_accuracy: 0.6970\n",
      "Epoch 241/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2116 - accuracy: 0.6894 - val_loss: 0.2216 - val_accuracy: 0.6818\n",
      "Epoch 242/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2084 - accuracy: 0.7014 - val_loss: 0.2267 - val_accuracy: 0.6818\n",
      "Epoch 243/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.2083 - accuracy: 0.7031 - val_loss: 0.2116 - val_accuracy: 0.7121\n",
      "Epoch 244/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2095 - accuracy: 0.6809 - val_loss: 0.2108 - val_accuracy: 0.7121\n",
      "Epoch 245/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.2136 - accuracy: 0.6724 - val_loss: 0.2236 - val_accuracy: 0.6970\n",
      "Epoch 246/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.2125 - accuracy: 0.6758 - val_loss: 0.2088 - val_accuracy: 0.7273\n",
      "Epoch 247/300\n",
      "586/586 [==============================] - 0s 44us/sample - loss: 0.1999 - accuracy: 0.6997 - val_loss: 0.2176 - val_accuracy: 0.7273\n",
      "Epoch 248/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.2062 - accuracy: 0.6911 - val_loss: 0.2162 - val_accuracy: 0.6970\n",
      "Epoch 249/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2181 - accuracy: 0.6826 - val_loss: 0.2164 - val_accuracy: 0.6818\n",
      "Epoch 250/300\n",
      "586/586 [==============================] - 0s 43us/sample - loss: 0.2031 - accuracy: 0.7082 - val_loss: 0.2279 - val_accuracy: 0.6667\n",
      "Epoch 251/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2069 - accuracy: 0.6928 - val_loss: 0.2207 - val_accuracy: 0.7121\n",
      "Epoch 252/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.1974 - accuracy: 0.7014 - val_loss: 0.2212 - val_accuracy: 0.6818\n",
      "Epoch 253/300\n",
      "586/586 [==============================] - 0s 56us/sample - loss: 0.2176 - accuracy: 0.6553 - val_loss: 0.2208 - val_accuracy: 0.7273\n",
      "Epoch 254/300\n",
      "586/586 [==============================] - 0s 59us/sample - loss: 0.2019 - accuracy: 0.6928 - val_loss: 0.2195 - val_accuracy: 0.6970\n",
      "Epoch 255/300\n",
      "586/586 [==============================] - 0s 59us/sample - loss: 0.2123 - accuracy: 0.6792 - val_loss: 0.2179 - val_accuracy: 0.7273\n",
      "Epoch 256/300\n",
      "586/586 [==============================] - 0s 60us/sample - loss: 0.2044 - accuracy: 0.7014 - val_loss: 0.2144 - val_accuracy: 0.7121\n",
      "Epoch 257/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.2103 - accuracy: 0.6877 - val_loss: 0.2185 - val_accuracy: 0.7424\n",
      "Epoch 258/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2096 - accuracy: 0.7031 - val_loss: 0.2156 - val_accuracy: 0.6970\n",
      "Epoch 259/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.2065 - accuracy: 0.6997 - val_loss: 0.2224 - val_accuracy: 0.7121\n",
      "Epoch 260/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.1925 - accuracy: 0.7116 - val_loss: 0.2137 - val_accuracy: 0.7121\n",
      "Epoch 261/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.2075 - accuracy: 0.6775 - val_loss: 0.2055 - val_accuracy: 0.6667\n",
      "Epoch 262/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.1926 - accuracy: 0.7082 - val_loss: 0.2130 - val_accuracy: 0.6970\n",
      "Epoch 263/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.1945 - accuracy: 0.7014 - val_loss: 0.2078 - val_accuracy: 0.6970\n",
      "Epoch 264/300\n",
      "586/586 [==============================] - 0s 53us/sample - loss: 0.2062 - accuracy: 0.6792 - val_loss: 0.2088 - val_accuracy: 0.6970\n",
      "Epoch 265/300\n",
      "586/586 [==============================] - 0s 51us/sample - loss: 0.1929 - accuracy: 0.7184 - val_loss: 0.2115 - val_accuracy: 0.7121\n",
      "Epoch 266/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.1957 - accuracy: 0.6911 - val_loss: 0.2138 - val_accuracy: 0.6970\n",
      "Epoch 267/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.1957 - accuracy: 0.7082 - val_loss: 0.2164 - val_accuracy: 0.7121\n",
      "Epoch 268/300\n",
      "586/586 [==============================] - 0s 50us/sample - loss: 0.1954 - accuracy: 0.7014 - val_loss: 0.2172 - val_accuracy: 0.6970\n",
      "Epoch 269/300\n",
      "586/586 [==============================] - 0s 58us/sample - loss: 0.1936 - accuracy: 0.6809 - val_loss: 0.2192 - val_accuracy: 0.7273\n",
      "Epoch 270/300\n",
      "586/586 [==============================] - 0s 54us/sample - loss: 0.1924 - accuracy: 0.7031 - val_loss: 0.2155 - val_accuracy: 0.7273\n",
      "Epoch 271/300\n",
      "586/586 [==============================] - 0s 57us/sample - loss: 0.1847 - accuracy: 0.7304 - val_loss: 0.2254 - val_accuracy: 0.7121\n",
      "Epoch 272/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.1888 - accuracy: 0.7184 - val_loss: 0.2112 - val_accuracy: 0.7121\n",
      "Epoch 273/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.1957 - accuracy: 0.6962 - val_loss: 0.2181 - val_accuracy: 0.7121\n",
      "Epoch 274/300\n",
      "586/586 [==============================] - 0s 55us/sample - loss: 0.1862 - accuracy: 0.7201 - val_loss: 0.2112 - val_accuracy: 0.6818\n",
      "Epoch 275/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.1856 - accuracy: 0.7099 - val_loss: 0.2163 - val_accuracy: 0.6818\n",
      "Epoch 276/300\n",
      "586/586 [==============================] - 0s 69us/sample - loss: 0.1899 - accuracy: 0.7048 - val_loss: 0.2166 - val_accuracy: 0.6667\n",
      "Epoch 277/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "586/586 [==============================] - 0s 53us/sample - loss: 0.1931 - accuracy: 0.6980 - val_loss: 0.2138 - val_accuracy: 0.6818\n",
      "Epoch 278/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.1892 - accuracy: 0.7133 - val_loss: 0.2155 - val_accuracy: 0.6970\n",
      "Epoch 279/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.1863 - accuracy: 0.7184 - val_loss: 0.2177 - val_accuracy: 0.6667\n",
      "Epoch 280/300\n",
      "586/586 [==============================] - 0s 46us/sample - loss: 0.1889 - accuracy: 0.7082 - val_loss: 0.2181 - val_accuracy: 0.6667\n",
      "Epoch 281/300\n",
      "586/586 [==============================] - 0s 45us/sample - loss: 0.1830 - accuracy: 0.7150 - val_loss: 0.2136 - val_accuracy: 0.7121\n",
      "Epoch 282/300\n",
      "586/586 [==============================] - 0s 48us/sample - loss: 0.1827 - accuracy: 0.7270 - val_loss: 0.2143 - val_accuracy: 0.6970\n",
      "Epoch 283/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.1880 - accuracy: 0.7133 - val_loss: 0.2202 - val_accuracy: 0.6818\n",
      "Epoch 284/300\n",
      "586/586 [==============================] - 0s 65us/sample - loss: 0.1797 - accuracy: 0.7184 - val_loss: 0.2198 - val_accuracy: 0.6515\n",
      "Epoch 285/300\n",
      "586/586 [==============================] - 0s 69us/sample - loss: 0.1741 - accuracy: 0.7270 - val_loss: 0.2226 - val_accuracy: 0.6970\n",
      "Epoch 286/300\n",
      "586/586 [==============================] - 0s 70us/sample - loss: 0.1848 - accuracy: 0.7389 - val_loss: 0.2242 - val_accuracy: 0.6515\n",
      "Epoch 287/300\n",
      "586/586 [==============================] - 0s 56us/sample - loss: 0.1857 - accuracy: 0.7321 - val_loss: 0.2285 - val_accuracy: 0.6970\n",
      "Epoch 288/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.1793 - accuracy: 0.7509 - val_loss: 0.2289 - val_accuracy: 0.7121\n",
      "Epoch 289/300\n",
      "586/586 [==============================] - 0s 58us/sample - loss: 0.1901 - accuracy: 0.6911 - val_loss: 0.2211 - val_accuracy: 0.7121\n",
      "Epoch 290/300\n",
      "586/586 [==============================] - 0s 57us/sample - loss: 0.1860 - accuracy: 0.7253 - val_loss: 0.2249 - val_accuracy: 0.6970\n",
      "Epoch 291/300\n",
      "586/586 [==============================] - 0s 63us/sample - loss: 0.1756 - accuracy: 0.7133 - val_loss: 0.2216 - val_accuracy: 0.6667\n",
      "Epoch 292/300\n",
      "586/586 [==============================] - 0s 60us/sample - loss: 0.1784 - accuracy: 0.7235 - val_loss: 0.2255 - val_accuracy: 0.6818\n",
      "Epoch 293/300\n",
      "586/586 [==============================] - 0s 57us/sample - loss: 0.1901 - accuracy: 0.7031 - val_loss: 0.2259 - val_accuracy: 0.6818\n",
      "Epoch 294/300\n",
      "586/586 [==============================] - 0s 52us/sample - loss: 0.1825 - accuracy: 0.7167 - val_loss: 0.2246 - val_accuracy: 0.6818\n",
      "Epoch 295/300\n",
      "586/586 [==============================] - 0s 58us/sample - loss: 0.1790 - accuracy: 0.7253 - val_loss: 0.2198 - val_accuracy: 0.7121\n",
      "Epoch 296/300\n",
      "586/586 [==============================] - 0s 47us/sample - loss: 0.1841 - accuracy: 0.7116 - val_loss: 0.2224 - val_accuracy: 0.6515\n",
      "Epoch 297/300\n",
      "586/586 [==============================] - 0s 49us/sample - loss: 0.1763 - accuracy: 0.7184 - val_loss: 0.2278 - val_accuracy: 0.6667\n",
      "Epoch 298/300\n",
      "586/586 [==============================] - 0s 54us/sample - loss: 0.1847 - accuracy: 0.7167 - val_loss: 0.2233 - val_accuracy: 0.7121\n",
      "Epoch 299/300\n",
      "586/586 [==============================] - 0s 62us/sample - loss: 0.1772 - accuracy: 0.7321 - val_loss: 0.2237 - val_accuracy: 0.6970\n",
      "Epoch 300/300\n",
      "586/586 [==============================] - 0s 63us/sample - loss: 0.1792 - accuracy: 0.7321 - val_loss: 0.2239 - val_accuracy: 0.6970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a22aa6be0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=300, validation_split=0.1, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0725 15:20:17.522644 4606760384 training.py:504] Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116/116 [==============================] - 0s 44us/sample - loss: 0.1778 - accuracy: 0.7241\n",
      "test loss, test acc: [0.1777748835497889, 0.7241379]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
